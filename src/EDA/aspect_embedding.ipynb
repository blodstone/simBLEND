{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8de60c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfbcefbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoConfig, AutoTokenizer,  AutoModelForMaskedLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch.nn.functional as F\n",
    "from data_modules.mind_aspect_data import AspectNewsBatch, MINDAspectDataModule\n",
    "from modules.aspect_enc import AspectRepr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9363a3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('nickprock/ModernBERT-large-sts')\n",
    "text = [\n",
    "    \"Unexpected Reasons Why You Might Have A Fever\tFeeling a little heated? It's typically nothing to worry about â€• especially if you're sick. However, other issues can cause your temperature to rise.\",\n",
    "    \"Brain scans don't lie: The minds of girls and boys are equal in math\tSeveral studies have already debunked the myth that boys are innately better at math than girls, and new brain images offer more proof\",\n",
    "    \"Ken Fisher has a side bet on a risky corner of Wall Street\tBillionaire Ken Fisher made his name and fortune picking stocks. But over the years he's also become a huge player in an arcane -- and controversial -- corner of Wall Street: exchange-traded notes.\"\n",
    "]\n",
    "query_texts = [\n",
    "    \"Australia deports woman to Vietnam over smuggled pork\tCANBERRA, Australia (AP)   Australia for the first time has canceled a tourist's visa over undeclared food as the country tries to keep itself free of African swine fever.\",\n",
    "    \"Exercising More Past 60 Cuts Risk Of Stroke, Heart Disease\tThe elderly are supposed to exercise more if they want to live longer, according to a new study.\",\n",
    "    \"FaceTime chats and the chance to change IU's fortunes lured Tiawan Mullen to Indiana\tTiawan Mullen had nearly two dozen offers; he chose IU for the chance 'to change everything.'\"\n",
    "]\n",
    "# Create 5 queries: some similar, some dissimilar to the texts in `text`\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenized_texts = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
    "tokenized_texts = tokenized_texts.to(device)\n",
    "tokenized_queries = tokenizer(query_texts, padding=True, truncation=True, return_tensors='pt')\n",
    "tokenized_queries = tokenized_queries.to(device)\n",
    "batch_texts = AspectNewsBatch(news={'text': tokenized_texts}, labels=None)\n",
    "batch_queries = AspectNewsBatch(news={'text': tokenized_queries}, labels=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "131ddd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0856, 0.8785, 0.0258],\n",
      "        [0.0853, 0.5327, 0.1114],\n",
      "        [0.1065, 0.1819, 0.0523]])\n",
      "torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "model = AspectRepr.load_from_checkpoint('/home/users1/hardy/hardy/project/vae/checkpoints/aspect_cat_sts-epoch=17-val_loss=1.7645.ckpt')  # Replace with your checkpoint path\n",
    "model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_embeddings = model(batch_texts)[0].cpu()\n",
    "    query_embeddings = model(batch_queries)[0].cpu()\n",
    "# Compute cosine similarity between each query and each text embedding\n",
    "# text_embeddings = text_embeddings - text_embeddings.mean(dim=0, keepdim=True)\n",
    "# query_embeddings = query_embeddings - query_embeddings.mean(dim=0, keepdim=True)\n",
    "similarity_matrix__cat_sts = F.cosine_similarity(\n",
    "    text_embeddings.unsqueeze(1),  # shape: (5, 1, hidden_dim)\n",
    "    query_embeddings.unsqueeze(0),   # shape: (1, 5, hidden_dim)\n",
    "    dim=-1\n",
    ")  # shape: (5, 5)\n",
    "\n",
    "print(similarity_matrix__cat_sts)\n",
    "print(similarity_matrix__cat_sts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "297e63b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4911, 0.5301, 0.5458],\n",
      "        [0.3858, 0.4863, 0.5057],\n",
      "        [0.5627, 0.4540, 0.6066]])\n",
      "torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "model = AspectRepr.load_from_checkpoint(plm_name='nickprock/ModernBERT-large-sts')  # Replace with your checkpoint path\n",
    "model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_embeddings = model(batch_texts)[0].cpu()\n",
    "    query_embeddings = model(batch_queries)[0].cpu()\n",
    "# Compute cosine similarity between each query and each text embedding\n",
    "# text_embeddings = text_embeddings - text_embeddings.mean(dim=0, keepdim=True)\n",
    "# query_embeddings = query_embeddings - query_embeddings.mean(dim=0, keepdim=True)\n",
    "similarity_matrix_sts = F.cosine_similarity(\n",
    "    text_embeddings.unsqueeze(1),  # shape: (5, 1, hidden_dim)\n",
    "    query_embeddings.unsqueeze(0),   # shape: (1, 5, hidden_dim)\n",
    "    dim=-1\n",
    ")  # shape: (5, 5)\n",
    "\n",
    "print(similarity_matrix_sts)\n",
    "print(similarity_matrix_sts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "055e4e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0608,  0.8463,  0.0968],\n",
      "        [ 0.2161,  0.5786,  0.2868],\n",
      "        [-0.1097,  0.2985,  0.1948]])\n",
      "torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "model = AspectRepr.load_from_checkpoint('/home/users1/hardy/hardy/project/vae/checkpoints/aspect_cat-epoch=23-val_loss=0.1483.ckpt')  # Replace with your checkpoint path\n",
    "model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_embeddings = model(batch_texts)[0].cpu()\n",
    "    query_embeddings = model(batch_queries)[0].cpu()\n",
    "# Compute cosine similarity between each query and each text embedding\n",
    "# text_embeddings = text_embeddings - text_embeddings.mean(dim=0, keepdim=True)\n",
    "# query_embeddings = query_embeddings - query_embeddings.mean(dim=0, keepdim=True)\n",
    "similarity_matrix__cat = F.cosine_similarity(\n",
    "    text_embeddings.unsqueeze(1),  # shape: (5, 1, hidden_dim)\n",
    "    query_embeddings.unsqueeze(0),   # shape: (1, 5, hidden_dim)\n",
    "    dim=-1\n",
    ")  # shape: (5, 5)\n",
    "\n",
    "print(similarity_matrix__cat)\n",
    "print(similarity_matrix__cat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "aa7e58a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation: -0.11734262\n",
      "p-value: 0.7636776054503343\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Flatten the matrices to 1D arrays\n",
    "flat_sts = similarity_matrix_sts.flatten().numpy()\n",
    "flat_cat_sts = similarity_matrix__cat_sts.flatten().numpy()\n",
    "\n",
    "# Compute Pearson correlation\n",
    "corr, p_value = pearsonr(flat_sts, flat_cat_sts)\n",
    "print(\"Pearson correlation:\", corr)\n",
    "print(\"p-value:\", p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6bd4077f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7774, 0.4657, 0.5165, 0.4903, 0.4036, 0.4034, 0.4367],\n",
      "        [0.4476, 0.9223, 0.3963, 0.5841, 0.3943, 0.3956, 0.4614],\n",
      "        [0.5431, 0.5897, 0.3965, 0.7107, 0.4081, 0.4401, 0.4518],\n",
      "        [0.5719, 0.6070, 0.4026, 0.9721, 0.4467, 0.3462, 0.4645],\n",
      "        [0.4630, 0.6549, 0.4077, 0.5781, 0.4336, 0.3734, 0.4852],\n",
      "        [0.4351, 0.3932, 0.2597, 0.3564, 0.3210, 0.9712, 0.3807],\n",
      "        [0.4661, 0.5065, 0.3939, 0.5059, 0.3938, 0.3232, 0.4728]])\n",
      "torch.Size([7, 7])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Compute cosine similarity between each query and each text embedding\n",
    "similarity_matrix = F.cosine_similarity(\n",
    "    text_embeddings.unsqueeze(1),  # shape: (5, 1, hidden_dim)\n",
    "    query_embeddings.unsqueeze(0),   # shape: (1, 5, hidden_dim)\n",
    "    dim=-1\n",
    ")  # shape: (5, 5)\n",
    "\n",
    "print(similarity_matrix)\n",
    "print(similarity_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dcab9297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5747,  0.3004,  0.5111,  0.1979,  0.2439,  0.5848,  0.2136],\n",
      "        [ 0.2841,  0.7830,  0.1380,  0.5863,  0.1190,  0.3566,  0.2754],\n",
      "        [ 0.1427,  0.4481,  0.0093,  0.8223,  0.0607,  0.4142,  0.0732],\n",
      "        [ 0.1785,  0.4405,  0.0284,  0.9386,  0.0641,  0.4007,  0.0352],\n",
      "        [ 0.1281,  0.5254, -0.0067,  0.7838,  0.0876,  0.3851,  0.0666],\n",
      "        [ 0.4195,  0.3483,  0.2975,  0.3971,  0.2493,  0.9454,  0.2192],\n",
      "        [ 0.4221,  0.3663,  0.3560,  0.3708,  0.1997,  0.4435,  0.4579]])\n",
      "torch.Size([7, 7])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Compute cosine similarity between each query and each text embedding\n",
    "similarity_matrix = F.cosine_similarity(\n",
    "    text_embeddings.unsqueeze(1),  # shape: (5, 1, hidden_dim)\n",
    "    query_embeddings.unsqueeze(0),   # shape: (1, 5, hidden_dim)\n",
    "    dim=-1\n",
    ")  # shape: (5, 5)\n",
    "\n",
    "print(similarity_matrix)\n",
    "print(similarity_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105b1e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4373, -0.2444,  0.1924, -0.2702,  0.0163, -0.0143, -0.0626],\n",
      "        [-0.1138,  0.4727, -0.1028,  0.0852, -0.0200, -0.2261, -0.1003],\n",
      "        [ 0.0440,  0.0956, -0.0512,  0.1628, -0.0326, -0.1186, -0.0892],\n",
      "        [-0.0504, -0.0135, -0.0799,  0.6442, -0.0599, -0.2328, -0.1963],\n",
      "        [-0.1419,  0.3530, -0.0690,  0.1569, -0.0131, -0.2202, -0.0735],\n",
      "        [-0.1203, -0.2846, -0.0717, -0.3513,  0.0657,  0.7930, -0.0609],\n",
      "        [-0.1026, -0.1626,  0.1232, -0.2273,  0.0182, -0.1675,  0.4878]])\n",
      "torch.Size([7, 7])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b5040e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4373, -0.2444,  0.1924, -0.2702,  0.0163, -0.0143, -0.0626],\n",
      "        [-0.1138,  0.4727, -0.1028,  0.0852, -0.0200, -0.2261, -0.1003],\n",
      "        [ 0.0440,  0.0956, -0.0512,  0.1628, -0.0326, -0.1186, -0.0892],\n",
      "        [-0.0504, -0.0135, -0.0799,  0.6442, -0.0599, -0.2328, -0.1963],\n",
      "        [-0.1419,  0.3530, -0.0690,  0.1569, -0.0131, -0.2202, -0.0735],\n",
      "        [-0.1203, -0.2846, -0.0717, -0.3513,  0.0657,  0.7930, -0.0609],\n",
      "        [-0.1026, -0.1626,  0.1232, -0.2273,  0.0182, -0.1675,  0.4878]])\n",
      "torch.Size([7, 7])\n"
     ]
    }
   ],
   "source": [
    "text_embeddings = text_embeddings - text_embeddings.mean(dim=0, keepdim=True)\n",
    "query_embeddings = query_embeddings - query_embeddings.mean(dim=0, keepdim=True)\n",
    "# Compute cosine similarity between each query and each text embedding\n",
    "similarity_matrix = F.cosine_similarity(\n",
    "    text_embeddings.unsqueeze(1),  # shape: (5, 1, hidden_dim)\n",
    "    query_embeddings.unsqueeze(0),   # shape: (1, 5, hidden_dim)\n",
    "    dim=-1\n",
    ")  # shape: (5, 5)\n",
    "\n",
    "print(similarity_matrix)\n",
    "print(similarity_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75a5ecd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name =  \"nickprock/ModernBERT-large-sts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af435f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(model_name)\n",
    "config.update({\"device_map\": \"cuda:0\", \"torch_dtype\": torch.bfloat16})\n",
    "text_encoder = AutoModel.from_pretrained(model_name, config=config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88fc5508",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer(\n",
    "            text, return_tensors=\"pt\", return_token_type_ids=False, padding=True, truncation=True\n",
    "        )\n",
    "\n",
    "outputs = text_encoder(**tokens)\n",
    "text_embeddings = outputs.last_hidden_state[:, 0, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e3760e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_tokens = tokenizer(\n",
    "            query_texts, return_tensors=\"pt\", return_token_type_ids=False, padding=True, truncation=True\n",
    "        )\n",
    "query_outputs = text_encoder(**query_tokens)\n",
    "query_embeddings = query_outputs.last_hidden_state[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b291fd07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7774, 0.4657, 0.5165, 0.4903, 0.4036, 0.4034],\n",
      "        [0.4476, 0.9223, 0.3963, 0.5841, 0.3943, 0.3956],\n",
      "        [0.5431, 0.5897, 0.3965, 0.7107, 0.4081, 0.4401],\n",
      "        [0.5719, 0.6070, 0.4026, 0.9721, 0.4467, 0.3462],\n",
      "        [0.4630, 0.6549, 0.4077, 0.5781, 0.4336, 0.3734],\n",
      "        [0.4351, 0.3932, 0.2597, 0.3564, 0.3210, 0.9712]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "torch.Size([6, 6])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Compute cosine similarity between each query and each text embedding\n",
    "similarity_matrix = F.cosine_similarity(\n",
    "    text_embeddings.unsqueeze(1),  # shape: (5, 1, hidden_dim)\n",
    "    query_embeddings.unsqueeze(0),   # shape: (1, 5, hidden_dim)\n",
    "    dim=-1\n",
    ")  # shape: (5, 5)\n",
    "\n",
    "print(similarity_matrix)\n",
    "print(similarity_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cf1beb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5457701744d04c6fb07650047193b48d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"Qwen/Qwen3-Embedding-4B\"\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "config.update({\"attn_implementation\": \"flash_attention_2\", \"device_map\": \"auto\", \"torch_dtype\": torch.bfloat16})\n",
    "text_encoder = AutoModel.from_pretrained(model_name, config=config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d571c3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(\n",
    "            text, return_tensors=\"pt\", return_token_type_ids=False, padding=True, truncation=True\n",
    "        )\n",
    "\n",
    "outputs = text_encoder(**tokens)\n",
    "text_embeddings = outputs.last_hidden_state[:, 0, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7749f2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_tokens = tokenizer(\n",
    "            query_texts, return_tensors=\"pt\", return_token_type_ids=False, padding=True, truncation=True\n",
    "        )\n",
    "query_outputs = text_encoder(**query_tokens)\n",
    "query_embeddings = query_outputs.last_hidden_state[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45ae974f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.9182, 0.7954, 0.7907, 1.0000, 0.9313],\n",
      "        [0.8799, 0.9548, 0.8821, 0.9011, 0.8799, 0.9004],\n",
      "        [0.7686, 0.8889, 0.8978, 0.9188, 0.7686, 0.8149],\n",
      "        [0.7907, 0.8817, 0.8911, 1.0000, 0.7907, 0.8351],\n",
      "        [0.8653, 0.9553, 0.8971, 0.9160, 0.8653, 0.8965],\n",
      "        [0.9313, 0.9085, 0.8730, 0.8351, 0.9313, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "torch.Size([6, 6])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Compute cosine similarity between each query and each text embedding\n",
    "similarity_matrix = F.cosine_similarity(\n",
    "    text_embeddings.unsqueeze(1),  # shape: (5, 1, hidden_dim)\n",
    "    query_embeddings.unsqueeze(0),   # shape: (1, 5, hidden_dim)\n",
    "    dim=-1\n",
    ")  # shape: (5, 5)\n",
    "\n",
    "print(similarity_matrix)\n",
    "print(similarity_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e30d81b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model = \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mQwen/Qwen3-Embedding-4B\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mattn_implementation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mflash_attention_2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdevice_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtorch_dtype\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbfloat16\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpadding_side\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mleft\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hardy/project/vae/.venv/lib64/python3.13/site-packages/sentence_transformers/SentenceTransformer.py:309\u001b[39m, in \u001b[36mSentenceTransformer.__init__\u001b[39m\u001b[34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[39m\n\u001b[32m    300\u001b[39m         model_name_or_path = __MODEL_HUB_ORGANIZATION__ + \u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m + model_name_or_path\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentence_transformer_model(\n\u001b[32m    303\u001b[39m     model_name_or_path,\n\u001b[32m    304\u001b[39m     token,\n\u001b[32m   (...)\u001b[39m\u001b[32m    307\u001b[39m     local_files_only=local_files_only,\n\u001b[32m    308\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m309\u001b[39m     modules, \u001b[38;5;28mself\u001b[39m.module_kwargs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_sbert_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    321\u001b[39m     modules = \u001b[38;5;28mself\u001b[39m._load_auto_model(\n\u001b[32m    322\u001b[39m         model_name_or_path,\n\u001b[32m    323\u001b[39m         token=token,\n\u001b[32m   (...)\u001b[39m\u001b[32m    330\u001b[39m         config_kwargs=config_kwargs,\n\u001b[32m    331\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hardy/project/vae/.venv/lib64/python3.13/site-packages/sentence_transformers/SentenceTransformer.py:1808\u001b[39m, in \u001b[36mSentenceTransformer._load_sbert_model\u001b[39m\u001b[34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code, local_files_only, model_kwargs, tokenizer_kwargs, config_kwargs)\u001b[39m\n\u001b[32m   1805\u001b[39m \u001b[38;5;66;03m# Try to initialize the module with a lot of kwargs, but only if the module supports them\u001b[39;00m\n\u001b[32m   1806\u001b[39m \u001b[38;5;66;03m# Otherwise we fall back to the load method\u001b[39;00m\n\u001b[32m   1807\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1808\u001b[39m     module = \u001b[43mmodule_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1809\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   1810\u001b[39m     module = module_class.load(model_name_or_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hardy/project/vae/.venv/lib64/python3.13/site-packages/sentence_transformers/models/Transformer.py:81\u001b[39m, in \u001b[36mTransformer.__init__\u001b[39m\u001b[34m(self, model_name_or_path, max_seq_length, model_args, tokenizer_args, config_args, cache_dir, do_lower_case, tokenizer_name_or_path, backend)\u001b[39m\n\u001b[32m     78\u001b[39m     config_args = {}\n\u001b[32m     80\u001b[39m config, is_peft_model = \u001b[38;5;28mself\u001b[39m._load_config(model_name_or_path, cache_dir, backend, config_args)\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_peft_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m max_seq_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmodel_max_length\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tokenizer_args:\n\u001b[32m     84\u001b[39m     tokenizer_args[\u001b[33m\"\u001b[39m\u001b[33mmodel_max_length\u001b[39m\u001b[33m\"\u001b[39m] = max_seq_length\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hardy/project/vae/.venv/lib64/python3.13/site-packages/sentence_transformers/models/Transformer.py:181\u001b[39m, in \u001b[36mTransformer._load_model\u001b[39m\u001b[34m(self, model_name_or_path, config, cache_dir, backend, is_peft_model, **model_args)\u001b[39m\n\u001b[32m    179\u001b[39m     \u001b[38;5;28mself\u001b[39m._load_mt5_model(model_name_or_path, config, cache_dir, **model_args)\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m     \u001b[38;5;28mself\u001b[39m.auto_model = \u001b[43mAutoModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_peft_model:\n\u001b[32m    186\u001b[39m     \u001b[38;5;28mself\u001b[39m._load_peft_model(model_name_or_path, config, cache_dir, **model_args, **adapter_only_kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hardy/project/vae/.venv/lib64/python3.13/site-packages/transformers/models/auto/auto_factory.py:547\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    544\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mquantization_config\u001b[39m\u001b[33m\"\u001b[39m] = kwargs_orig[\u001b[33m\"\u001b[39m\u001b[33mquantization_config\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    546\u001b[39m has_remote_code = \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[33m\"\u001b[39m\u001b[33mauto_map\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config.auto_map\n\u001b[32m--> \u001b[39m\u001b[32m547\u001b[39m has_local_code = \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_model_mapping\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    548\u001b[39m trust_remote_code = resolve_trust_remote_code(\n\u001b[32m    549\u001b[39m     trust_remote_code, pretrained_model_name_or_path, has_local_code, has_remote_code\n\u001b[32m    550\u001b[39m )\n\u001b[32m    551\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mtrust_remote_code\u001b[39m\u001b[33m\"\u001b[39m] = trust_remote_code\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hardy/project/vae/.venv/lib64/python3.13/site-packages/transformers/models/auto/auto_factory.py:792\u001b[39m, in \u001b[36m_LazyAutoMapping.keys\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    790\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mkeys\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    791\u001b[39m     mapping_keys = [\n\u001b[32m--> \u001b[39m\u001b[32m792\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_attr_from_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    793\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m key, name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._config_mapping.items()\n\u001b[32m    794\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._model_mapping.keys()\n\u001b[32m    795\u001b[39m     ]\n\u001b[32m    796\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping_keys + \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m._extra_content.keys())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hardy/project/vae/.venv/lib64/python3.13/site-packages/transformers/models/auto/auto_factory.py:787\u001b[39m, in \u001b[36m_LazyAutoMapping._load_attr_from_module\u001b[39m\u001b[34m(self, model_type, attr)\u001b[39m\n\u001b[32m    785\u001b[39m module_name = model_type_to_module_name(model_type)\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m module_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._modules:\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m     \u001b[38;5;28mself\u001b[39m._modules[module_name] = \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodule_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtransformers.models\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m getattribute_from_module(\u001b[38;5;28mself\u001b[39m._modules[module_name], attr)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib64/python3.13/importlib/__init__.py:88\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     86\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     87\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1387\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1331\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:935\u001b[39m, in \u001b[36m_load_unlocked\u001b[39m\u001b[34m(spec)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1022\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1173\u001b[39m, in \u001b[36mget_code\u001b[39m\u001b[34m(self, fullname)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1241\u001b[39m, in \u001b[36m_cache_bytecode\u001b[39m\u001b[34m(self, source_path, bytecode_path, data)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1266\u001b[39m, in \u001b[36mset_data\u001b[39m\u001b[34m(self, path, data, _mode)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:206\u001b[39m, in \u001b[36m_write_atomic\u001b[39m\u001b[34m(path, data, mode)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer(\n",
    "    \"Qwen/Qwen3-Embedding-4B\",\n",
    "    model_kwargs={\"attn_implementation\": \"flash_attention_2\", \"device_map\": \"auto\", \"torch_dtype\": torch.bfloat16},\n",
    "    tokenizer_kwargs={\"padding_side\": \"left\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d2469c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embeddings = model.encode(query_texts)\n",
    "document_embeddings = model.encode(text, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57af238a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AutoModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model = \u001b[43mAutoModel\u001b[49m.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33mQwen/Qwen3-Embedding-4B\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'AutoModel' is not defined"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\"Qwen/Qwen3-Embedding-4B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a1075b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7837, 0.4297, 0.4470, 0.5315, 0.4269, 0.2790],\n",
      "        [0.3465, 0.8922, 0.4865, 0.5247, 0.5822, 0.1880],\n",
      "        [0.4432, 0.4472, 0.3335, 0.4556, 0.3956, 0.1639],\n",
      "        [0.3883, 0.4413, 0.5320, 0.9348, 0.4025, 0.1411],\n",
      "        [0.4616, 0.3829, 0.3040, 0.3573, 0.3999, 0.2902],\n",
      "        [0.3018, 0.1434, 0.1413, 0.1385, 0.1579, 0.9171]])\n"
     ]
    }
   ],
   "source": [
    "similarity = model.similarity(query_embeddings, document_embeddings)\n",
    "print(similarity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
